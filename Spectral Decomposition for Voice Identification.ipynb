{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69018993-1925-4152-8eec-e28759d82ea6",
   "metadata": {},
   "source": [
    "Text Independent Speaker Identification is the task of identifying and separating individual speakers in a given audio sample. This technique has been applied in various fields, for example it is used in certain banking and home security services as a way to verify the identity of users beyond traditional methods. Unlabeled voice registrations are used as input for a model whose objective is to identify the different voices based on individual characteristics. This dictionary of voices can then be compared with new samples to identify the speaker.\n",
    "\n",
    "One of the ways this kind of task has been completed is by extracting the frequency spectrum for the audio sample and then using a Gaussian Mixture Model (GMM) to distinguish different people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af796e-47e4-4f1d-aa1d-6bcf4f0152ef",
   "metadata": {},
   "source": [
    "The data set used was sourced from the openslr project at the link: https://www.openslr.org/45/. It was uploaded by the Surfingtech Company that extracted it from a private bigger data set. It is made up of samples of a few seconds each for 10 speakers. For each speaker there are about 350 recordings in the form of .wav files. To decrease the computing time, the first 20 recordings for each speaker where used for training, for a total of 200 training samples, and the next 5 for testing, so 50 testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1853bf-4071-4f26-ac8a-3fbc629bfb8e",
   "metadata": {},
   "source": [
    "To obtain an effective modeling of the audio samples, it isn't possible to just pass the samples to a GMM for fitting and then complete the prediction, it is instead necessary to process the signals and extract from them certain features which can than be used to successfully train a predictor model, in this case a GMM.\n",
    "\n",
    "This process of feature extraction is done in this case through the Mel-Frequency Cepstrum (MFC). A cepstrum is sometimes described as the spectrum of a spectrum, because it refers to the representation of a signal obtained by first applying the Fourier Transform to such signal and then reapplying it to the log of the transformed signal. This is also equivalent, ignoring a scaling factor, to calculating the inverse transform:\n",
    "$$\\begin{equation}\n",
    "C=\\left|\\mathcal{F}^{-1}\\left\\{\\log \\left(|\\mathcal{F}\\{f(t)\\}|^2\\right)\\right\\}\\right|^2\n",
    "\\quad \\sim \\quad\n",
    "C=\\left|\\mathcal{F}\\left\\{\\log \\left(|\\mathcal{F}\\{f(t)\\}|^2\\right)\\right\\}\\right|^2\n",
    "\\end{equation}$$\n",
    "\n",
    "This representation in a new time domain, different from the original one of the signal  ùëì(ùë°) , is particularly useful when analyzing the human voice because characteristics like the voice pitch and the so called formants (peaks in the spectrum caused by resonances in the human vocal tract) are more easily distinguishable.\n",
    "\n",
    "The Mel-Frequency Cepstrum is a variant of this process, where the powers of the spectrum obtained with the first transform are mapped on the mel (melody) scale, which is a pitch scale based on the human ear.\n",
    "$$m = 2595 \\log_{10} \\left( 1 + \\frac{f}{700} \\right)$$\n",
    "\n",
    "\n",
    "Than a Discrete Cosine Transform is applied and the amplitudes of the resulting spectrum are the Mel-Frequency Cepstrum Coefficients or MFCCs. The \"delta\" are than values computed as differences between coefficients as in the following formula. They should offer a description of how the sounds vary in time and are considered important for voice analysis.\n",
    "$$\\begin{equation}\n",
    "d_t=\\frac{\\sum_{n=1}^N n\\left(c_{t+n}-c_{t-n}\\right)}{2 \\sum_{n=1}^N n^2}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e8275c-060d-4a14-820a-c1a67945813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f5ef6-6b0f-468a-80e3-6dca95c31bbb",
   "metadata": {},
   "source": [
    "The computation of the MFC coefficients and of the delta happens through two functions, calculate_delta and extract_features. calculate_delta accepts in input the array of coefficients extracted for each time frame by the extract_features function and, for each time frame (each row), extracts the +1/-1 and +2/-2 indices to use to obtain the correct coefficients for each delta. For every coefficient of every time window, we have 1 delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d01e01-cf09-4638-80b6-9dd5ef930067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta(array):\n",
    "    \"\"\"Calculate and returns the delta of given feature vector matrix\"\"\"\n",
    "\n",
    "    rows,cols = array.shape     #Number of cols and rows in the input array, representing respectively the coefficients and the time frames.\n",
    "    deltas = np.zeros((rows,20)) #Initialize array to store the delta\n",
    "    N = 2 #Temporal distance of the coefficients to be considered, so t-1, t+1 and t-2, t+2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= N:\n",
    "            if i-j < 0: #Expect for the first frame...\n",
    "              first =0\n",
    "            else:\n",
    "              first = i-j\n",
    "            if i+j > rows-1:  #... and the last one\n",
    "                second = rows-1\n",
    "            else:\n",
    "                second = i+j \n",
    "            index.append((second,first)) #first adding n=1, then n=2\n",
    "            j+=1\n",
    "        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10 #computing delta for the i-th row\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedb9c1-bb39-4557-a5ff-6b7509200cb8",
   "metadata": {},
   "source": [
    "The extract_features function takes as input the raw audio file with its sampling rate. It then applies the mfcc method with a window size of 25ms and a hop lenght of 10ms. The Fast-Fourier Transform is executed with a precision of 1200 points (for example, if the file sample rate is 44100Hz, then this would mean a frequency resolution of about 37 Hz). After the second (Discrete Cosine) Transform is applied to the log of the data mapped on the mel scale, the first 20 coefficients are kept. This should be sufficient to keep enough details for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde1738e-cbed-45cb-a35d-99c7de6096fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio,rate):\n",
    "    \"\"\"extract 20 dim mfcc features from an audio, performs CMS and combines \n",
    "    delta to make it 40 dim feature vector\"\"\"    \n",
    "    \n",
    "    mfcc_feature = mfcc.mfcc(audio,rate, 0.025, 0.01,20,nfft = 1200, appendEnergy = True)    \n",
    "    mfcc_feature = preprocessing.scale(mfcc_feature)\n",
    "    delta = calculate_delta(mfcc_feature)\n",
    "    combined = np.hstack((mfcc_feature,delta)) \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa09632-da08-47ed-881b-77834b55bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78a492-193d-4106-97c8-ed211994dfe7",
   "metadata": {},
   "source": [
    "As we said the training data comes from a data set sourced by openslr. We have taken 25 sample for each speaker and used 20 for training and 5 for testing. The \"source\" variable contains the path to the folder with the training samples, \"train_file\" is a .txt file with the name of each training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04c5f7d-9f2c-4d83-9111-2ea05613c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to training data\n",
    "source   = \"/Users/chiaracangelosi/Desktop/Speaker-Recognition-Using-GMM-MFCC-Python3-master/Voice_Samples_Training/\"   \n",
    "\n",
    "#path where training speakers will be saved\n",
    "dest = \"Trained_Speech_Models/\"\n",
    "train_file = \"Voice_Samples_Training_Path.txt\"        \n",
    "file_paths = open(train_file,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c30c96-16a2-470f-ade7-d584f800caab",
   "metadata": {},
   "source": [
    "In the next cell we loop through the audio samples of each speaker (20 per speaker) and extract coefficients+deltas using the previously defined functions. The feature array for each sample is stacked row-wise. This array becomes the training data for a GMM witch will then be a representation of that specific person voice. The GMM function is the one from sklearn.mixture. 5 gaussian distributions are used for each model, which proved to be an effective number. Each model is initialized three different times in a tentative to avoid local optima. Each speaker model is saved separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e89ca9d-358a-45dc-8851-ddc46742b73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-001/f1_1.wav\n",
      "f1-001/f1_2.wav\n",
      "f1-001/f1_3.wav\n",
      "f1-001/f1_4.wav\n",
      "f1-001/f1_5.wav\n",
      "f1-001/f1_6.wav\n",
      "f1-001/f1_7.wav\n",
      "f1-001/f1_8.wav\n",
      "f1-001/f1_9.wav\n",
      "f1-001/f1_10.wav\n",
      "f1-001/f1_11.wav\n",
      "f1-001/f1_12.wav\n",
      "f1-001/f1_13.wav\n",
      "f1-001/f1_14.wav\n",
      "f1-001/f1_15.wav\n",
      "f1-001/f1_16.wav\n",
      "f1-001/f1_17.wav\n",
      "f1-001/f1_18.wav\n",
      "f1-001/f1_19.wav\n",
      "f1-001/f1_20.wav\n",
      "f2-002/f2_1.wav\n",
      "f2-002/f2_2.wav\n",
      "f2-002/f2_3.wav\n",
      "f2-002/f2_4.wav\n",
      "f2-002/f2_5.wav\n",
      "f2-002/f2_6.wav\n",
      "f2-002/f2_7.wav\n",
      "f2-002/f2_8.wav\n",
      "f2-002/f2_9.wav\n",
      "f2-002/f2_10.wav\n",
      "f2-002/f2_11.wav\n",
      "f2-002/f2_12.wav\n",
      "f2-002/f2_13.wav\n",
      "f2-002/f2_14.wav\n",
      "f2-002/f2_15.wav\n",
      "f2-002/f2_16.wav\n",
      "f2-002/f2_17.wav\n",
      "f2-002/f2_18.wav\n",
      "f2-002/f2_19.wav\n",
      "f2-002/f2_20.wav\n",
      "f3-003/f3_1.wav\n",
      "f3-003/f3_2.wav\n",
      "f3-003/f3_3.wav\n",
      "f3-003/f3_4.wav\n",
      "f3-003/f3_5.wav\n",
      "f3-003/f3_6.wav\n",
      "f3-003/f3_7.wav\n",
      "f3-003/f3_8.wav\n",
      "f3-003/f3_9.wav\n",
      "f3-003/f3_10.wav\n",
      "f3-003/f3_11.wav\n",
      "f3-003/f3_12.wav\n",
      "f3-003/f3_13.wav\n",
      "f3-003/f3_14.wav\n",
      "f3-003/f3_15.wav\n",
      "f3-003/f3_16.wav\n",
      "f3-003/f3_17.wav\n",
      "f3-003/f3_18.wav\n",
      "f3-003/f3_19.wav\n",
      "f3-003/f3_20.wav\n",
      "f4-004/f4_1.wav\n",
      "f4-004/f4_2.wav\n",
      "f4-004/f4_3.wav\n",
      "f4-004/f4_4.wav\n",
      "f4-004/f4_5.wav\n",
      "f4-004/f4_6.wav\n",
      "f4-004/f4_7.wav\n",
      "f4-004/f4_8.wav\n",
      "f4-004/f4_9.wav\n",
      "f4-004/f4_10.wav\n",
      "f4-004/f4_11.wav\n",
      "f4-004/f4_12.wav\n",
      "f4-004/f4_13.wav\n",
      "f4-004/f4_14.wav\n",
      "f4-004/f4_15.wav\n",
      "f4-004/f4_16.wav\n",
      "f4-004/f4_17.wav\n",
      "f4-004/f4_18.wav\n",
      "f4-004/f4_19.wav\n",
      "f4-004/f4_20.wav\n",
      "f5-005/f5_1.wav\n",
      "f5-005/f5_2.wav\n",
      "f5-005/f5_3.wav\n",
      "f5-005/f5_4.wav\n",
      "f5-005/f5_5.wav\n",
      "f5-005/f5_6.wav\n",
      "f5-005/f5_7.wav\n",
      "f5-005/f5_8.wav\n",
      "f5-005/f5_9.wav\n",
      "f5-005/f5_10.wav\n",
      "f5-005/f5_11.wav\n",
      "f5-005/f5_12.wav\n",
      "f5-005/f5_13.wav\n",
      "f5-005/f5_14.wav\n",
      "f5-005/f5_15.wav\n",
      "f5-005/f5_16.wav\n",
      "f5-005/f5_17.wav\n",
      "f5-005/f5_18.wav\n",
      "f5-005/f5_19.wav\n",
      "f5-005/f5_20.wav\n",
      "m1-006/m1_1.wav\n",
      "m1-006/m1_2.wav\n",
      "m1-006/m1_3.wav\n",
      "m1-006/m1_4.wav\n",
      "m1-006/m1_5.wav\n",
      "m1-006/m1_6.wav\n",
      "m1-006/m1_7.wav\n",
      "m1-006/m1_8.wav\n",
      "m1-006/m1_9.wav\n",
      "m1-006/m1_10.wav\n",
      "m1-006/m1_11.wav\n",
      "m1-006/m1_12.wav\n",
      "m1-006/m1_13.wav\n",
      "m1-006/m1_14.wav\n",
      "m1-006/m1_15.wav\n",
      "m1-006/m1_16.wav\n",
      "m1-006/m1_17.wav\n",
      "m1-006/m1_18.wav\n",
      "m1-006/m1_19.wav\n",
      "m1-006/m1_20.wav\n",
      "m2-007/m2_1.wav\n",
      "m2-007/m2_2.wav\n",
      "m2-007/m2_3.wav\n",
      "m2-007/m2_4.wav\n",
      "m2-007/m2_5.wav\n",
      "m2-007/m2_6.wav\n",
      "m2-007/m2_7.wav\n",
      "m2-007/m2_8.wav\n",
      "m2-007/m2_9.wav\n",
      "m2-007/m2_10.wav\n",
      "m2-007/m2_11.wav\n",
      "m2-007/m2_12.wav\n",
      "m2-007/m2_13.wav\n",
      "m2-007/m2_14.wav\n",
      "m2-007/m2_15.wav\n",
      "m2-007/m2_16.wav\n",
      "m2-007/m2_17.wav\n",
      "m2-007/m2_18.wav\n",
      "m2-007/m2_19.wav\n",
      "m2-007/m2_20.wav\n",
      "m3-008/m3_1.wav\n",
      "m3-008/m3_2.wav\n",
      "m3-008/m3_3.wav\n",
      "m3-008/m3_4.wav\n",
      "m3-008/m3_5.wav\n",
      "m3-008/m3_6.wav\n",
      "m3-008/m3_7.wav\n",
      "m3-008/m3_8.wav\n",
      "m3-008/m3_9.wav\n",
      "m3-008/m3_10.wav\n",
      "m3-008/m3_11.wav\n",
      "m3-008/m3_12.wav\n",
      "m3-008/m3_13.wav\n",
      "m3-008/m3_14.wav\n",
      "m3-008/m3_15.wav\n",
      "m3-008/m3_16.wav\n",
      "m3-008/m3_17.wav\n",
      "m3-008/m3_18.wav\n",
      "m3-008/m3_19.wav\n",
      "m3-008/m3_20.wav\n",
      "m4-009/m4_1.wav\n",
      "m4-009/m4_2.wav\n",
      "m4-009/m4_3.wav\n",
      "m4-009/m4_4.wav\n",
      "m4-009/m4_5.wav\n",
      "m4-009/m4_6.wav\n",
      "m4-009/m4_7.wav\n",
      "m4-009/m4_8.wav\n",
      "m4-009/m4_9.wav\n",
      "m4-009/m4_10.wav\n",
      "m4-009/m4_11.wav\n",
      "m4-009/m4_12.wav\n",
      "m4-009/m4_13.wav\n",
      "m4-009/m4_14.wav\n",
      "m4-009/m4_15.wav\n",
      "m4-009/m4_16.wav\n",
      "m4-009/m4_17.wav\n",
      "m4-009/m4_18.wav\n",
      "m4-009/m4_19.wav\n",
      "m4-009/m4_20.wav\n",
      "m5-010/m5_1.wav\n",
      "m5-010/m5_2.wav\n",
      "m5-010/m5_3.wav\n",
      "m5-010/m5_4.wav\n",
      "m5-010/m5_5.wav\n",
      "m5-010/m5_6.wav\n",
      "m5-010/m5_7.wav\n",
      "m5-010/m5_8.wav\n",
      "m5-010/m5_9.wav\n",
      "m5-010/m5_10.wav\n",
      "m5-010/m5_11.wav\n",
      "m5-010/m5_12.wav\n",
      "m5-010/m5_13.wav\n",
      "m5-010/m5_14.wav\n",
      "m5-010/m5_15.wav\n",
      "m5-010/m5_16.wav\n",
      "m5-010/m5_17.wav\n",
      "m5-010/m5_18.wav\n",
      "m5-010/m5_19.wav\n",
      "m5-010/m5_20.wav\n"
     ]
    }
   ],
   "source": [
    "#counter for how many files have been processed per speaker\n",
    "count = 1\n",
    "# Extracting features for each speaker (20 files per speakers)\n",
    "features = np.asarray(()) #extracted features\n",
    "for path in file_paths:    \n",
    "    path = path.strip() #Removes whitespaces\n",
    "    print (path)\n",
    "    \n",
    "    # read the audio\n",
    "    sr,audio = read(source+path) #sr is the file sampling rate, which is automatically sourced by the scipy read function\n",
    "    \n",
    "    # extract 40 dimensional MFCC & delta MFCC features\n",
    "    vector   = extract_features(audio,sr)\n",
    "    \n",
    "    if features.size == 0:\n",
    "        features = vector\n",
    "    else:\n",
    "        features = np.vstack((features, vector)) #stacks new features below existing ones (row-wise)\n",
    "        \n",
    "    # After features of 5 files of a single speaker have been concatenated, then we are ready to train the model\n",
    "    if count == 20:    \n",
    "        gmm = GMM(n_components = 5, covariance_type='diag',n_init = 3) #Creates a GMM with 5 components, initializing it 3 times to search for a good fit\n",
    "        gmm.fit(features)\n",
    "        \n",
    "        #saving the trained model using the python module Pickle, used for serializing (converting an object into a file so it can be saved and loaded later)\n",
    "        #and deserializing objects (reading that file and reconstructing the object in memory)\n",
    "        picklefile = path.split(\"-\")[0]+\".gmm\" #Taking the name and adding .gmm\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))  #Saving the GMM parameters for each speaker\n",
    "        print ('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape) \n",
    "\n",
    "        #reset variables\n",
    "        features = np.asarray(())\n",
    "\n",
    "        #reset counter\n",
    "        count = 0\n",
    "        \n",
    "        #increment counter\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bb78f-4859-4da7-ac9c-3dd5b2b4275a",
   "metadata": {},
   "source": [
    "In the final section \"source\" becomes the folder with the audio samples we want to test. We created two possibilities, to either check a single file against our models to check the best correspondence or to pass a whole library of files to assess the overall performances of voice classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011c1345-4e8f-41cc-bfbe-2050b2417815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "#path to training data\n",
    "source   = \"/Users/chiaracangelosi/Desktop/Speaker-Recognition-Using-GMM-MFCC-Python3-master/Testing_Audio/\"\n",
    "\n",
    "#path where training speakers will be saved \n",
    "#path with the previously trained models to check against\n",
    "modelpath = \"/Users/chiaracangelosi/Desktop/Speaker-Recognition-Using-GMM-MFCC-Python3-master/Trained_Speech_Models/\"\n",
    "\n",
    "gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "              os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "#Load the Gaussian gender Models\n",
    "models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]  #list containing all loaded models\n",
    "speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname        #it extracts speaker names from .gmm filenames\n",
    "              in gmm_files]\n",
    "\n",
    "error = 0\n",
    "total_sample = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95da226-bb27-4682-bc8b-bfcb573ca311",
   "metadata": {},
   "source": [
    "The user can choose single audio testing or batch audio testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "693a9ec0-871a-47f1-a3fc-894da9125258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Audio :  f1_21.wav\n",
      "\tdetected as -  f1\n",
      "Testing Audio :  f1_22.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f1_23.wav\n",
      "\tdetected as -  f1\n",
      "Testing Audio :  f1_24.wav\n",
      "\tdetected as -  f1\n",
      "Testing Audio :  f1_25.wav\n",
      "\tdetected as -  f1\n",
      "Testing Audio :  f2_21.wav\n",
      "\tdetected as -  f2\n",
      "Testing Audio :  f2_22.wav\n",
      "\tdetected as -  f3\n",
      "Testing Audio :  f2_23.wav\n",
      "\tdetected as -  f2\n",
      "Testing Audio :  f2_24.wav\n",
      "\tdetected as -  f2\n",
      "Testing Audio :  f2_25.wav\n",
      "\tdetected as -  f2\n",
      "Testing Audio :  f3_21.wav\n",
      "\tdetected as -  f3\n",
      "Testing Audio :  f3_22.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f3_23.wav\n",
      "\tdetected as -  f3\n",
      "Testing Audio :  f3_24.wav\n",
      "\tdetected as -  f3\n",
      "Testing Audio :  f3_25.wav\n",
      "\tdetected as -  m5\n",
      "Testing Audio :  f4_21.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f4_22.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f4_23.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f4_24.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f4_25.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f5_21.wav\n",
      "\tdetected as -  f4\n",
      "Testing Audio :  f5_22.wav\n",
      "\tdetected as -  f5\n",
      "Testing Audio :  f5_23.wav\n",
      "\tdetected as -  f5\n",
      "Testing Audio :  f5_24.wav\n",
      "\tdetected as -  f5\n",
      "Testing Audio :  f5_25.wav\n",
      "\tdetected as -  f5\n",
      "Testing Audio :  m1_21.wav\n",
      "\tdetected as -  m1\n",
      "Testing Audio :  m1_22.wav\n",
      "\tdetected as -  m1\n",
      "Testing Audio :  m1_23.wav\n",
      "\tdetected as -  m1\n",
      "Testing Audio :  m1_24.wav\n",
      "\tdetected as -  m1\n",
      "Testing Audio :  m1_25.wav\n",
      "\tdetected as -  m1\n",
      "Testing Audio :  m2_21.wav\n",
      "\tdetected as -  m2\n",
      "Testing Audio :  m2_22.wav\n",
      "\tdetected as -  m2\n",
      "Testing Audio :  m2_23.wav\n",
      "\tdetected as -  m2\n",
      "Testing Audio :  m2_24.wav\n",
      "\tdetected as -  m2\n",
      "Testing Audio :  m2_25.wav\n",
      "\tdetected as -  m2\n",
      "Testing Audio :  m3_21.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m3_22.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m3_23.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m3_24.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m3_25.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m4_21.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m4_22.wav\n",
      "\tdetected as -  m4\n",
      "Testing Audio :  m4_23.wav\n",
      "\tdetected as -  m4\n",
      "Testing Audio :  m4_24.wav\n",
      "\tdetected as -  m3\n",
      "Testing Audio :  m4_25.wav\n",
      "\tdetected as -  m4\n",
      "Testing Audio :  m5_21.wav\n",
      "\tdetected as -  m5\n",
      "Testing Audio :  m5_22.wav\n",
      "\tdetected as -  m5\n",
      "Testing Audio :  m5_23.wav\n",
      "\tdetected as -  m5\n",
      "Testing Audio :  m5_24.wav\n",
      "\tdetected as -  m5\n",
      "Testing Audio :  m5_25.wav\n",
      "\tdetected as -  f1\n",
      "8 50.0\n",
      "The Accuracy Percentage for the current testing Performance with MFCC + GMM is :  84.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\")\n",
    "take=int(input().strip())\n",
    "\n",
    "#This is the single audio testing case. The system first reads the user-provided audio file and extracts MFC features, which capture the characteristics of \n",
    "#the speaker's voice. It then computes log-likelihood scores for each trained GMM (Gaussian Mixture Model), measuring how well the extracted features match \n",
    "#each stored speaker profile. The speaker with the highest likelihood score is selected as the best match. Finally, the system prints the identified speaker‚Äôs \n",
    "#name, providing the recognition result.\n",
    "\n",
    "if take == 1:\n",
    "    print (\"Enter the File name from the sample with .wav notation :\") #Specifying the file to be tested\n",
    "    path =input().strip()\n",
    "    print ((\"Testing Audio : \",path))\n",
    "    sr,audio = read(source + path)\n",
    "    vector   = extract_features(audio,sr) #Extracting the file features\n",
    "    \n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum() #Summing the log-likelihoods is equivalent to multiplying the probabilities of each interval sample (25ms)\n",
    "    \n",
    "    winner = np.argmax(log_likelihood)\n",
    "    print (\"\\tThe person in the given audio sample is detected as - \", speakers[winner])\n",
    "\n",
    "    time.sleep(1.0)\n",
    "\n",
    "#This is the batch audio testing case. The system reads all test audio files listed in Testing_audio_Path.txt and extracts MFCC features for each file. \n",
    "#It then computes log-likelihood scores for all trained GMM (Gaussian Mixture Model) models, determining how well each model matches the extracted features. \n",
    "#The detected speaker is compared with the expected speaker name to check for correctness. Finally the accuracy is computed.\n",
    "\n",
    "elif take == 0:\n",
    "    test_file = \"/Users/chiaracangelosi/Desktop/Speaker-Recognition-Using-GMM-MFCC-Python3-master/Testing_audio_Path.txt\"        \n",
    "    file_paths = open(test_file,'r')\n",
    "    # Read the test directory and get the list of test audio files \n",
    "    total_sample = 0.0  \n",
    "    error = 0  \n",
    "    for path in file_paths:   \n",
    "        total_sample+= 1.0\n",
    "        path=path.strip()\n",
    "        print(\"Testing Audio : \", path)\n",
    "        sr,audio = read(source + path) #read audio file\n",
    "        vector   = extract_features(audio,sr) #extract MFCC features\n",
    "        log_likelihood = np.zeros(len(models)) \n",
    "        for i in range(len(models)):\n",
    "            gmm    = models[i]  #checking with each model one by one\n",
    "            scores = np.array(gmm.score(vector)) #compute the likelihood score\n",
    "            log_likelihood[i] = scores.sum() \n",
    "        winner=np.argmax(log_likelihood) #best matching speaker\n",
    "        print (\"\\tdetected as - \", speakers[winner])\n",
    "        checker_name = path.split(\"_\")[0]\n",
    "        if speakers[winner] != checker_name:  #check if the name is correct\n",
    "            error += 1\n",
    "        time.sleep(1.0)\n",
    "    print (error, total_sample)\n",
    "    accuracy = ((total_sample - error) / total_sample) * 100\n",
    "\n",
    "    print (\"The Accuracy Percentage for the current testing Performance with MFCC + GMM is : \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fb818-9174-42aa-8280-056e3be5004f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
